Data Engineering Examples

This repo includes a few small examples for doing ETL (Extract, Transform, Load) using PySpark.
It’s mostly for learning and experimenting; something simple that can run locally and show how data engineering workflows fit together.

What it does

Right now, there’s just one main example.
It reads a CSV file, does some basic transformations (like grouping and averaging values), and writes the result to another CSV file.
Nothing fancy — just a clean, minimal ETL pipeline from start to finish.

Files included
pyspark_etl_example.py   -> main ETL script
input_data.csv           -> sample input data
output_data.csv          -> output generated by the script
run_etl.bat              -> optional batch file for running on Windows
README.md                -> this file

How to run it

Make sure you have:

Python 3.11 or newer

Java JDK 11 (Temurin or OpenJDK works fine)

PySpark installed (no need to install Spark separately)

If you’re on Windows, you’ll also need winutils.exe (already set up in this example)

Create and activate a virtual environment:

python -m venv venv
venv\Scripts\activate


Install the required packages:

pip install pyspark pandas


Run the ETL script:

python pyspark_etl_example.py


You’ll see the results printed in the console, and an output_data.csv file will appear in the same folder.

Example

Input (input_data.csv):

category	value
A	10
A	20
B	30
C	40

Output (output_data.csv):

category	avg_value
A	15.0
B	30.0
C	40.0

Notes

This project is just for learning and testing out ideas — not production code.
I might add more examples later, like joins, cleaning messy data, or writing to S3 or a SQL database.

Author

Danial Farooq
