Data Engineering Examples

Hi! Iâ€™m Danial Farooq, and this repo is a collection of my data engineering projects and experiments.
Iâ€™ve worked with tools like PySpark, Hadoop, and Python to build and test different kinds of data pipelines, from small ETL examples to larger transformations.

ğŸ§  Whatâ€™s Inside
1. PySpark ETL Example

This project shows how to use PySpark for a simple ETL (Extract, Transform, Load) process.
It reads input data, performs transformations like grouping and aggregation, and saves the results as a clean CSV file.

Files included:

pyspark_etl_example.py â€“ main ETL script

input_data.csv â€“ sample input data

output_data.csv â€“ generated output after processing

run_etl.bat â€“ batch file for quick execution on Windows

âš™ï¸ How to Run It

Make sure you have Python and Java installed, then run these commands:

# 1. Create and activate a virtual environment
py -3.11 -m venv venv
venv\Scripts\activate

# 2. Install dependencies
pip install pyspark pandas

# 3. Run the ETL script
python pyspark_etl_example.py


The processed output will be saved as output_data.csv in the same folder.

ğŸ’¬ Why I Built This

I built this project to put data engineering concepts into practice; focusing on how Spark handles large data transformations efficiently.
These examples help me sharpen my understanding of how distributed processing works in real-world data workflows.

ğŸš€ Whatâ€™s Next

I plan to extend this repo with:

More PySpark and Spark SQL examples

A small data pipeline project with scheduling and monitoring

Possibly a Dockerized version for easier deployment

ğŸ“« Connect with Me

If youâ€™re into data engineering or Spark too, Iâ€™d love to connect and share ideas!
